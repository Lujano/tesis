\chapter{Marco Teórico}
\label{capitulo3}
\lhead{Capítulo 3. \emph{Marco Teórico}}


\section{Introducción}

En esta sección se presentarán las bases teóricas que sustentan la implementación realizada en este trabajo. Se definirá el modelo de cámara utilizado, las transformaciones de cuerpo rígido, los detectores y descriptores de características, la extracción y filtrado de correspondencias, y los filtros de fusión inercial.


\section{Modelo pinhole de la cámara}
El modelo pinhole o de pequeña apertura de la cámara es una aproximación proyectiva en la que la medición tomada por la cámara (imagen) es el resultado de una transformación del espacio en tres dimensiones del mundo  al espacio en dos dimensiones de una imagen (${\rm I\!R}^3 \to {\rm I\!R}^2$). Este modelo de cámara es el modelo más simple y utilizado en las aplicaciones de odometría y SLAM visual.

La figura  \ref{imagen:modelo_pinhole}  presenta esta aproximación. En primer lugar se observa el sistema de referencia de la cámara el cual es presentado con los ejes $X_{c}$, $Y_{c}$ y $Z_{c}$. El eje $Z_{c}$ también es conocido como el eje óptico o principal de la cámara. Además, se presenta el plano de la imagen en la figuras \ref{imagen:modelo_pinhole} y \ref{imagen:modelo_pinhole2}, el cual es un plano que se encuentra ubicado en $z= f$, donde $f$ es la distancia focal de la cámara en unidades de píxeles. En dicho plano se puede observar el sistema de referencia de la imagen, el cual se representa con los ejes punteados $u$ y $v$. En la figura \ref{imagen:coordenadas_imagen} se puede apreciar este sistema de referencia y su correspondencia con la imagen.

Cabe destacar que el sistema de coordenadas de la imagen tiene como unidad el píxel, al igual que los ejes $x$ y $y$ del plano de la imagen, los cuales tienen la misma dirección que los ejes $X_{c}$ y $Y_{c}$ .


\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{modelo_pinhole}
	\caption[Modelo de apertura pequeña de la cámara]{Modelo de  pequeña apertura de la cámara\protect\footnotemark.}
	\label{imagen:modelo_pinhole}
\end{figure}
\footnotetext{\url{https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html}}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{coordenadas_imagen}
	\caption[Coordenadas de la imágen.]{Coordenadas de la imagen.}
	\label{imagen:coordenadas_imagen}
\end{figure}



\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{modelo_pinhole2}
	\caption[Esquema de apertura pequeña de la cámara]{Esquema de apertura pequeña de la cámara. Adaptado \protect\footnotemark.}
	\label{imagen:modelo_pinhole2}
\end{figure}
\footnotetext{\url{https://www.researchgate.net/figure/Schematic-view-of-a-pinhole-camera-The-image-plane-is-shown-in-front-of-the-camera_fig2_265190953}}

El sistema de referencia de la imagen se relaciona con los ejes $X_{c}$ y  $Y_{c}$ mediante el punto principal de la imagen p = $(cx, cy , f)$ , el cual  se encuentra referenciado al sistema de la cámara. Los parámetros $cx$ y $cy$ son conocidos como el centro de la imagen y sus unidades están en píxeles. 


Finalmente, la cámara realiza una medida proyectiva del punto $P = (X, Y, Z)$ en el plano de la imagen. 
Este punto es transformado de las coordenadas en ${\rm I\!R}^3$ del mundo a las coordenadas en ${\rm I\!R}^2$ de la imagen mediante:

\begin{equation}
\begin{matrix} \left[ \begin{matrix} u \\ v \end{matrix} \right] =\left[ \begin{matrix} \frac { X.{ f } }{ Z } +{ c }_{ x } \\ \frac { Y.{ f } }{ Z } \quad +{ c }_{ y } \end{matrix} \right]  \end{matrix}
\label{eq:ProyeccionCAMSimple}
\end{equation}



En este modelo también se puede considerar que pueden existir diferentes distancias focales $f_{x}$ y $f_{y}$  en los ejes $x$ y $y$ de las coordenadas de la imagen.  Finalmente la proyección del punto P en las coordenadas de la imagen viene dada por el píxel de coordenadas $(u, v)$ mediante:

\begin{equation}
\left[ \begin{matrix} u \\ v \end{matrix} \right] =\left[ \begin{matrix} \frac { X.{ f }_{ x } }{ Z } +{ c }_{ x } \\ \frac { Y.{ f }_{ y } }{ Z } \quad +{ c }_{ y } \end{matrix} \right] 
\label{eq:ProyeccionCAM} 
\end{equation}

La ecuación \ref{eq:ProyeccionCAM} representa en el modelo de apertura pequeña la proyección que efectúa la cámara de un punto 3D $(X, Y, Z)$ dando como resultado el píxel $(u, v)$.

De la misma forma, la reproyección del píxel al punto , que resulta en una transformación de ${\rm I\!R}^2 \to {\rm I\!R}^3$, viene dada por:

\begin{equation}
\left[ \begin{matrix} X \\ Y \\ Z \end{matrix} \right] =\left[ \begin{matrix} \frac { u-{ c }_{ x } }{ { f }_{ x } } .Z \\ \frac { v-{ c }_{ y } }{ { f }_{ y } } .Z \\ Z \end{matrix} \right] =\quad Z.\left[ \begin{matrix} \frac { u-{ c }_{ x } }{ { f }_{ x } }  \\ \frac { v-{ c }_{ y } }{ { f }_{ y } }  \\ 1 \end{matrix} \right]  
\label{eq:ReproyeccionCAM} 
\end{equation}

Cabe destacar que el conjunto de puntos de la forma $s*(X, Y, Z)$, donde $s$ es un factor de escala o de profundidad, forman la recta de color rojo visualizada en la figura \ref{imagen:modelo_pinhole}. A estos puntos de ${\rm I\!R}^3$ le corresponde el mismo píxel de proyección sobre la imagen $(u, v)$. Esto implica que de una única imagen no es posible extraer la información de la profundidad del punto proyectado. Para ello es necesario más de una imagen y emplear métodos como los de triangulación.


%
\section{Detectores y descriptores de características}

Esta sección presentará un resumen de la estudio de detectores y descriptores de características expuestos en \cite{victor}.

Los puntos característicos o "features" son regiones en una imagen que contienen patrones específicos, lo que hace que puedan ser fácilmente seguidos o ubicados en otra imagen. Tuytelaars y Mikolajczyk \cite{Tuytelaars} definen un punto característico local como \textit{``un patrón en la imagen que difiere de su vecindario directo''}. 

En este sentido, los detectores de características son los algoritmos que clasifican regiones de una imagen bajo un criterio determinado, generando puntos característicos.

La imagen \ref{imagen:features} muestra un ejemplo de esto. En ésta se puede observar que se toman seis áreas de interés. Analizando estos segmentos, vemos las regiones que \textbf{\textit{A}} y \textbf{\textit{B}} corresponden con superficies planas, lo que hace difícil  identificar a que región de la imagen original corresponde ya que pueden existir más de una correspondencia. Por otro lado, tenemos las regiones \textbf{\textit{C}} y \textbf{\textit{D}}, las cuales corresponden con bordes en la imagen, que si bien es posible limitar en gran medida el área de búsqueda hacia toda las regiones del mismo bordes, sigue siendo difícil acertar con la ubicación correcta en la imagen. Por el contrario, en las regiones \textbf{\textit{E}} y \textbf{\textit{F}} observamos esquinas de la imagen original, en cuyo caso se puede identificar  la ubicación exacta de la región en la imagen con relativa facilidad.

De esta forma, se puede evidenciar que deben un detector de características de tener ciertas propiedades que garanticen la generación de puntos característicos confiables bajo distintas condiciones y que además permitan la posibilidad de ser emparejados en diferentes imágenes.


De esta forma, se considera que los puntos característicos deben proporcionar la posibilidad de ser identificados en diferentes imágenes con el objetivo de emparejarlos.

Para alcanzar este objetivo los detectores y extractores de puntos característicos deben cumplir con ciertas propiedades que les permita funcionar bajo distintas condiciones, en concreto se busca que estos algoritmos cumplan con las siguientes propiedades:

\begin{itemize}
	\item \textbf{Robustez:} El algoritmo debe ser capaz de detectar la misma ubicación del punto característico independientemente ante cambios en la escala, rotación, traslación, iluminación, transformaciones geométricas, artefactos de compresión y ruido.
	
	\item \textbf{Repetibilidad:} El algoritmo debe ser capaz de detectar el mismo punto característico de la misma escena bajo cambios en el punto de vista.
	
	\item \textbf{Exactitud:} El detector debe localizar el punto característico de manera precisa (misma ubicación de píxel). Especialmente para tareas de alineación de imágenes.
	
	\item \textbf{Generalidad:} El algoritmo debe ser capaz de detectar puntos que pueden ser usadas en distintas aplicaciones, es decir, que detecte varios tipos de características (esquinas, burbujas, etc.)
	
	\item \textbf{Eficiencia:} El algoritmo debe ser capaz de detectar puntos característicos en nuevas imágenes a gran velocidad, para soportar aplicaciones en tiempo real.
	
	\item \textbf{Cantidad:} El algoritmo debe detectar todos, o casi todos los puntos característicos presentes en la imagen. 
	
\end{itemize}



\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{features}
	\caption[Caracterización de regiones en una imagen]{Caracterización de regiones en una imagen\protect\footnotemark}
	\label{imagen:features}
\end{figure}
\footnotetext{\url{https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_features_meaning/py_features_meaning.html}}
%https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_features_meaning/py_features_meaning.html

Basados en estas definiciones es necesario exponer el funcionamiento de los algoritmos detectores.  Siguiendo el ejemplo de la imagen \ref{imagen:features}, y basado en la detección de esquinas como regiones facilmente identificables,  en \textit{1988} nace el primer algoritmo de detección de puntos de interés llamado Detector de esquinas de Harris \cite{harris} (nombre original en inglés: Harris Corner Detector).

Este detector utiliza una ventana de búsqueda para calcular la diferencia de intensidad de una región con su entorno directo, por lo que una esquina  será detectada en aquella regiones que presenten una alta variación de intensidad, al desplazar la ventana en cualquier dirección. En la figura \ref{imagen:harris-window} se puede apreciar visualmente como funciona esta ventana de búsqueda.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{harris-window}
	\caption[Ventana de búsqueda para de detección de esquinas]{Ventana de búsqueda para de detección de esquina. Adaptado\protect\footnotemark}
	\label{imagen:harris-window}
\end{figure}
\footnotetext{\url{https://dsp.stackexchange.com/questions/14338/}}
%https://dsp.stackexchange.com/questions/14338/

Sin embargo, este dectector a pesar de no sufrir variaciones ante cambios de tralación y rotación, ya que las esquinas se mantienen siendo esquinas al rotarse y trasladarse, no es invariante ante los cambios de escala, como lo muestra la figura \ref{imagen:corner-scale}, donde una región considerada como esquina, se podría considerar plana si es ampliada.


\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{corner-scale}
	\caption[Efecto del escalado sobre las esquinas]{Efecto del escalado sobre las esquinas	\protect\footnotemark}
	\label{imagen:corner-scale}
\end{figure}
\footnotetext{\url{https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html}}
%https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html

El problema básico a resolver es utilizar ventanas más grandes para detectar esquinas más grande. Debido a esto, en \textit{2004 D. Lowe} crea el detector y descriptor \textit{SIFT} \cite{sift} (del inglés: Scale Invariant Feature Transform), en el que se utiliza una representación de la imagen en espacio y escala, la cual es definida por los parámetros $s$ y $\sigma$. La figura \ref{imagen:piramidal} presenta el esquema piramidal, en la que se tienen diferentes niveles. La base de la pirámide corresponde a la imagen en su resolución original, y a medida que se sube se tienen escalamientos de la imagen en resoluciones inferiores. Al utilizar SIFT, cada nivel es llamado octava, y a la imagen 

filtrado espacial en diferentes escalas. En cada escala se emplea un kernel  gaussiano con diferentes valores de varianza. Por ejemplo, un kernel gaussiano con baja varianza de un alto valor para pequeñas esquinas, mientras que para esquinas grandes es mejor utilizar varianzas elevadas. 


\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{piramidal1}
	\caption{Pirámide de imágenes con diferentes niveles.}
	\label{imagen:piramidal}
\end{figure}

Posteriormente se restan las imágenes para distintos pares de escalas con lo que se genera un esquema piramidal DoG (del inglés: Difference of Gaussians). la. Este proceso de detección se puede visualizar en la figura \ref{imagen:sift-escalas}.

Posteriormente se buscan los máximos locales a lo largo del espacio (coordenadas $x$ e $y$) para cada correspondiente escala, lo que genera una lista de valores $(x, y, \sigma)$ donde existe una potencial característica en $(x, y)$ en la escala $\sigma$. Por ejemplo, un pixel en la imagen es comparado con sus 8 vecinos tanto en la escala actual, como en la escala anterior y posterior en la pirámide. Si este píxel es un extremo local en una, es una característica potencial en 


Una vez se haya detectado la ubicación de los puntos característicos en la imagen, la información de la localidad de éste debe ser codificada y almacenada, logrando así un descriptor único de la región con el objetivo final de ubicarlo en otra imagen. Con este fin se desarrollaron los algoritmos descriptores, los cuales una vez tengan la ubicación de los puntos característicos se encargan de convertir la información de su alrededor en una serie de números, o un vector que permita diferenciar un punto clave de otro. Esta información también es necesaria que sea invariante ante las variable mencionadas previamente, para lograr una identificación eficiente del mismo punto en distintas imágenes bajo distintas condiciones.



\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{sift-escalas}
	\caption[Detector SIFT]{Detección de máximos en el espacio de la escala DoG, adaptado de \cite{sift}}
	\label{imagen:sift-escalas}
\end{figure}

En segundo lugar para la localización de puntos de interés, se descartan los puntos encontrados en el paso anterior que no superen cierto valor de umbral, es decir, que no estén lo suficientemente contrastados con su entorno. Con esta etapa el algoritmo sólo toma en cuenta los puntos claves mas fuertes por cada escala. Además, con el objetivo de eliminar los bordes suficientemente contrastados que no correspondan con esquinas, el algoritmo usa una matriz Hessiana para calcular las curvaturas principales, y así quedarse solo con esquinas.

Para garantizar la invarianza con respecto a la rotación, se toman los píxeles vecinos al punto clave y se calcula la magnitud y dirección del gradiente en esa región. Con esto se hace un histograma de la magnitud del gradiente en cada dirección, donde el pico mayor del histograma indica la orientación. En el caso que exista un pico mayor al 80\% del pico principal, este se utiliza para crear otro punto de interés en la misma posición pero con la distinta rotación.

Finalmente para crear el vector descriptor por cada punto clave se crea una matriz de 16x16 alrededor de éste, dividida en 4 subregiones de 4x4 píxeles con un histograma de orientaciones para cada uno. Seguidamente, el descriptor del punto será el vector con los valores de los histogramas de las regiones 4x4 concatenados. La figura \ref{imagen:descriptor} la representación del descriptor de SIFT.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{sift-descriptor}
	\caption[Descriptor SIFT]{Izquierda: imagen de gradientes. Derecha: descriptor del punto clave. de \cite{sift}}
	\label{imagen:descriptor}
\end{figure}


En el año 2006, un grupo de tres personas: Bay, H., Tuytelaars, T. and Van Gool, L. desarrollan \textit{SURF} \cite{surf}, el cual es un detector y descriptor de características basado en SIFT, pero con modificaciones que aumentan su velocidad de detección. Estos cambios sacrifican un poco de rendimiento y precisión, pero sin embargo lo hace mas provechoso para aplicaciones embebidas que demanden mayor velocidad de cómputo y menor uso de recursos, como por ejemplo \textit{SLAM}. El proceso para la extracción de características por parte de este algoritmo se compone de los siguientes pasos:

Como primer paso, en lugar de aproximar el laplaciano de Gauss \textit{LoG} (del inglés: Laplacian of Gaussians) con la diferencia de Gaussianas (DoG) como lo hace SIFT, este algoritmo aproxima LoG con cuadrados para promediar la imagen. La ventaja de aplicar filtros con cuadrados es que con la ayuda de imágenes integrales el cálculo computacional se reduce en gran medida.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{surf}
	\caption[Detector y descriptor SURF]{Izquierda: aproximación a la derivada de segundo orden del filtro gaussiano (derivada parcial en el eje \textit{y}) y su aproximación con un filtro cuadrado. Derecha: vector de orientación del descriptor. Adaptado de \cite{surf}}
	\label{imagen:surf}
\end{figure}

En función de identificar la orientación, el algoritmo utiliza la respuesta Wavelet Haar en horizontal y vertical en un vecindario de 6$s$ (donde $s$ es la escala evaluada) píxeles al rededor del punto de interés, Luego estas respuestas son representadas como puntos en el espacio, para luego calcular la orientación dominante con la suma de todos los resultados dentro de una ventana deslizante de apertura 60$^\circ$. En la figura \ref{imagen:surf} se puede visualizar en el lado izquierdo, la derivada de segundo orden del filtro gaussiano, seguida de su aproximación con un filtro cuadrado. Del lado derecho se ilustra el vector de orientación en función a la distribución de puntos estudiados.

El siguiente avance importante en los algoritmos de detección aparece en el año 2011 con \textit{ORB} \cite{orb} (del inglés: Oriented FAST and Rotated BRIEF), este utiliza una combinación del detector FAST (del inglés: Features from Accelerated Segment Test) y del descriptor BRIEF (del inglés: Binary Robust Independent Elementary Features), este nuevo algoritmo se caracteriza por su alta velocidad de procesamiento manteniendo un buen rendimiento, gracias al uso de un descriptor binario. 

Como se mencionó utiliza el algoritmo FAST, el cual consiste en encontrar esquinas evaluando los píxeles en un perímetro circular, de esta forma, un punto será detectado como esquina si la cantidad de píxeles de color opuesto al evaluado, supera cierto valor de umbral (ver izquierda en la figura \ref{imagen:orb}), posteriormente con el fin de aumentar la robustez, es aplicado el algoritmo de clasificación de esquinas de $Harris$. También se realiza con una estructura piramidal evaluando varias escalas (al igual que SIFT).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{orb}
	\caption[Detector y descriptor ORB]{izquierda: detección de esquinas usando FAST, de \cite{fast}. derecha: descriptor basado en BRIEF, adaptado de\protect\footnotemark }
	\label{imagen:orb}
\end{figure}

\footnotetext{\url{https://gilscvblog.com/2013/10/04/}}
Como el algoritmo FAST no toma en cuenta la orientación, en el ORB se modificó para que calculara la orientación de la siguiente forma: Se considera una región ubicada en el centro del punto estudiado, luego se calcula el centroide de la región en función a la intensidad de los puntos. De esta forma, la dirección del vector desde el punto central  hasta el centroide es asignado como vector de orientación. Observando a la derecha en la figura \ref{imagen:orb} se aprecia un ejemplo del lugar del centroide \textit{(C)} y del centro \textit{(O)} para una región en particular.

El descriptor utilizado para BRIEF es un descriptor binario y no vectorial. El mismo produce una palabra de $n$-bits usando el algoritmo \textit{Local Binay Tests} (LBT), el problema de esta representación es que no es muy robusta ante cambios en la rotación. Para resolver esto ORB utiliza la información de la orientación previamente calculada en el paso de detección para aplicar LBT en esa orientación.

Los algoritmos de detección que se mencionaron hasta este momento tienen una característica en común, y es que cuando trabajan con el esquema piramidal lo hacen bajo el espacio de escala Gaussiano, el cual es una instancia particular de difusión lineal. De esta forma, al utilizar este filtro no se respetan los limites naturales de los objetos y se difumina del mismo nivel toda la región de la imagen cuando se avanza entre niveles de escala.

Enfocándose en esta característica, en el año de \textit{2012} se desarrolla el detector y descriptor llamado KAZE \cite{kaze} por parte de \textit{Pablo Fernández Alcantarilla}. Este novedoso algoritmo opera completamente en un espacio de escala no lineal, y para ello utilizan un esquema de división de operadores aditivos (\textit{AOS}, del inglés: Additive Operator Splitting), que les permite obtener espacios de escala no lineales de forma eficiente. De este modo se puede realizar un difuminado localmente adaptativo, posibilitando que se remueva el ruido en las imágenes, manteniendo información importante sobre los bordes de los objetos al avanzar en el espacio de escala. En la figura \ref{imagen:kaze} se puede observar como afecta en los bordes de los objetos el aplicar un filtro de difusión lineal, y uno que no lo es, bajo el esquema propuesto por este algoritmo.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{kaze}
	\caption[Filtro no lineal propuesto por KAZE]{\textit{(A)}: imagen original, \textit{(B)} filtro lineal Gaussiano, \textit{(C)} filtro no lineal propuesto en KAZE, adaptada de \cite{kaze}}
	\label{imagen:kaze}
\end{figure}

Bajo este mismo esquema de difusión no lineal, el mismo autor en el año \textit{2013} desarrolla la versión acelerada de este algoritmo que recibe el nombre de \textit{A-KAZE} \cite{akaze} (del ingles: Accelerated KAZE). Esta mejora utiliza un esquema basado en difusión explícita rápida \textit{FED} (del ingles: Fast Explicit Difussion) en lugar de \textit{AOS}, el cual es un nuevo esquema piramidal que incrementa en gran medida la velocidad de cómputo para construir el espacio de escala no lineal.

Para el cálculo de la orientación el primer algoritmo \textit{KAZE} utiliza un descriptor para la orientación similar al que emplea SURF. Este encuentra la orientación dominante en un área circular de radio 6$s$ ($s$ corresponde con la escala), y para cada muestra del círculo se calcula la derivada de primer orden en las direcciones $x$ e $y$, y se ponderan con una gaussiana centrada en el punto de interés. Luego, las respuestas de estas derivadas son representadas como puntos en un espacio vectorial, donde la orientación dominante se haya sumando las respuestas dentro de un segmento de circulo deslizante con apertura de 60$^\circ$.

Por otro lado, la versión acelerada \textit{A-KAZE} emplea un descriptor basado en una versión modificada del algoritmo de diferencia local binaria \textit{LDB} \cite{ldb} (del ingles: Local Difference Binary), llamado M-LBD (del ingles: Modified Local Difference Binary), el cual aprovecha al máximo la información del espacio de escala no lineal. La modificación consiste en hacer un sub-muestreo de cada región que divide la zona del descriptor, en lugar de calcular el promedio de todos los píxeles de la región, es decir, se tienen muestras de cada subdivisión para distintas escalas.


\section{Emparejadores de puntos característicos}

En este punto ya hemos estudiado los distintos algoritmos que permiten encontrar y clasificar puntos de interés en las imágenes. Ahora bien, es necesario identificar cuales de estos puntos corresponden con la misma ubicación, tal y como podemos observar en la figura \ref*{imagen:match}. Para establecer esta relación se utilizan los algoritmos emparejadores de características, los cuales relacionan estos puntos en base a sus descriptores.

\begin{figure}[H]
	\centering
	\includegraphics[width=14cm]{match}
	\caption[Emparejamiento de puntos característicos]{Emparejamiento de puntos que corresponden a la misma ubicación, de \cite{comp-vision} }
	\label{imagen:match}
\end{figure}

El proceso para realizar el emparejamiento consiste en el siguiente: Teniendo un punto característico $P_{1}$ perteneciente a la imagen $1$, y por otro lado se teniendo un punto $P_{2}$ perteneciente a la imagen $2$, Se calcula la distancia entre los descriptores $D_{1}$ y $D_{2}$. En el caso de descriptores vectoriales, esta distancia corresponde con la distancia \textit{Euclidiana}, dada por la siguiente expresión:
\begin{displaymath}
D = \sqrt{ (v_{1}-q_{1})^2 + (v_{2}-q_{2})^2 + \cdots + (v_{n}-q_{n})^2 }
\end{displaymath}
Donde $v_{n}$ corresponden con los componentes del vector descriptor $D1$, $q_{n}$ con los componentes del vector descriptor $D2$, y $n$ en el numero de componentes de ambos vectores. Por otro lado, para los descriptores binarios, se calcula la distancia \textit{Hamming}, dada por la siguiente expresión:
\begin{displaymath}
D = ||D_{1} \oplus D_{2}||
\end{displaymath}
Este proceso se repite hasta tener la distancia de cada punto de la imagen $1$ con todos los puntos de la imagen $2$, y viceversa. Al tener estas distancias, los puntos se emparejarán si y solo si se cumplen las siguientes condiciones: 

\begin{enumerate}[label=(\roman*)]
	\item El punto $P_{1}$ presenta la mejor distancia con $P_{2}$, en relación a todos los puntos de la imagen $2$.
	
	\item El punto $P_{2}$ presenta la mejor distancia con $P_{1}$ en relación a todos los puntos de la imagen $1$.
\end{enumerate}

Este proceso es mejor conocido como emparejamiento por fuerza bruta, ya que se compara entre todos los puntos por la mejor pareja posible. Aún cuando se asegura obtener el mejor emparejamiento, siendo viable para trabajar con pocos datos, el hecho de probar todas los casos posibles cuando se tiene una gran cantidad de puntos, incrementa en gran medida el tiempo de cómputo. Para efectuar este proceso de una forma eficiente se desarrollaron algoritmos basados en la búsqueda de vecinos mas cercanos. En este sentido se cuenta con el algoritmo \textit{kd-forest} (abreviado del inglés: k-dimensional forest), el cual es una mejora del algoritmo \textit{kd-tree} (abreviado del inglés: k-dimensional tree) para mejor desempeño al usar vectores multidimensionales, implementado en la librería para la rápida aproximación de vecinos mas cercanos FLANN \cite{flann} (del inglés: Fast Library for Approximate Nearest Neighbors).

%\textit{kd-tree} es una estructura de datos de segmentación de espacios para organizar puntos en un espacio de k dimensiones
\section{Módulo comparativo}

Una vez se conocen los algoritmos que se proponen utilizar, es necesario comparar su rendimiento bajo distintas condiciones, de modo que se pueda seleccionar el indicado para cada tipo de aplicación. En este sentido, se pretende estudiar el rendimiento en base a los siguientes parámetros:

\begin{itemize}
	\item \textbf{Tiempo de ejecución:} Tiempo en el que se detectan y describen las características en dos imágenes con las mismas dimensiones.
	
	\item \textbf{Cantidad de puntos detectados:} Cantidad total de puntos detectados y emparejados en dos imágenes.
	
	\item \textbf{Cantidad de puntos emparejados:} Cantidad total de puntos emparejados correctamente luego de descartar parejas erróneas. 
\end{itemize}

Adicionalmente, se propone aplicar algunos métodos que permiten aumentar el rendimiento de los algoritmos de detección, así como también implementar un proceso que permita reducir o eliminar falsos positivos al emparejar puntos de interés. 

\section{ Orientación de un cuerpo en el espacio}

La representación de la orientación de un cuerpo en el espacio puede ser expresada mediante diferentes representaciones, entre las cuáles se destacan la representación mediante ángulos de Euler,  la representación en ángulos de Roll, Pitch y Yaw (RPY), la representación en cuaterniones, y la representación  matricial mediante matrices de rotación de 3x3. Las dos primeras son más utilizadas con fines de visualización de la orientación ya que es posible graficar cada uno de los ángulos del robot para observar la variación de la orientación del robot en el tiempo, mientras que la representación en cuaterniones y matricial son formas compactas que permiten obtener la orientación final del robot mediante la concatenación de operaciones matemáticas. En particular, la representación de cuaterniones se considera más eficiente que la matricial por poseer un numero menor de operaciones mátematicas para obtener una  orientación determinada. Todas estas representaciones poseen transformaciones equivalentes para pasar de una representación a otra. En la figura \ref{imagen:rpy} se presenta el sistema de representación en RPY, el cual es utilizado en este trabajo para la visualización de la orientación y su error de estimación.


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{RPY}
	\caption[Representación de la rotación como ángulos  RPY]{Representación de la rotación como ángulos RPY. La orientación del cuerpo se obtiene mediante la rotación de forma secuencial de los ejes de Roll, Pitch y Yaw en el sentido que se presenta en la figura.}
	\label{imagen:rpy}
\end{figure} 

En la representación RPY se puede definir cualquier rotación como la operación consecutiva de 3 rotaciones elementales:



\begin{itemize}
	\item Una rotación alrededor del eje X, definido entre ${-180}^{\circ }$ y ${180}^{\circ }$, a la que se le denomina roll o cabeceo $ \phi$. 
	\item Una rotación alrededor del eje Y, definido entre ${-90}^{\circ }$ y  ${90}^{\circ }$, a la que se le denomina pitch o alabeo $\theta$.
	\item Una rotación alrededor del eje Z, definido entre ${-180}^{\circ }$ y ${180}^{\circ }$, a la que se le denomina yaw o guiñada $\psi$.
	
	
\end{itemize}

\subsection{ Representación Matricial de la orientación }

En la representación matricial, la rotación de un cuerpo puede ser representada una matriz de 3x3:

\begin{equation}
\begin{matrix} R\quad =\quad \quad \begin{bmatrix} { r }_{ 11 } & { \quad r }_{ 12 } & { \quad r }_{ 13 } \\ { r }_{ 21 } & { \quad r }_{ 22 } & { \quad r }_{ 23 } \\ { r }_{ 31 } & { \quad r }_{ 32 } & { \quad r }_{ 33 } \end{bmatrix}\quad  \end{matrix}
\label{eq:MatrizRotacion}
\end{equation}

Esta matriz debe ser ortogonal de determinante uno:

\begin{equation}
{ R }^{ T }\quad =\quad { R }^{ -1 }
\label{eq:Propiedad1MatrizRotacion}
\end{equation}
\begin{equation}
det(R)\quad =\quad 1
\label{eq:Propiedad2MatrizRotacion}
\end{equation}


 Y  debido a estas propiedades  puede representar una función de transformación de (${\rm I\!R}^3 \to {\rm I\!R}^3$), ya que la multiplicación de una matriz rotacional con un punto perteneciente a ${\rm I\!R}^3$ genera otro punto de ${\rm I\!R}^3 $:
 
 \begin{equation}
 \begin{pmatrix} \begin{matrix} X' \\ Y' \\ Z' \end{matrix} \end{pmatrix}\quad =\quad R\quad \begin{pmatrix} \begin{matrix} X \\ Y \\ Z \end{matrix} \end{pmatrix}\quad 
 \label{eq:Propiedad3MatrizRotacion}
 \end{equation}
 
 
 También es posible obtener la orientación final de un cuerpo mediante la concatenación de matrices de rotación:
 
 
  \begin{equation}
 { R }_{ 3 }\quad =\quad { R }_{ 1 }{ R }_{ 2 }
 \label{eq:Propiedad4MatrizRotacion}
 \end{equation}
 
 Donde ${ R }_{ 1 }$ representa la orientación inicial, ${ R }_{ 2 }$ el cambio de orientación, y ${ R }_{ 3 }$ la orientación final.
 
 En la figura \ref{imagen:rpy2matriz} se presenta la forma en la que es posible obtener la matriz rotacional a partir de los ángulos RPY.
 
 \begin{figure}[H]
 	\centering
 	\includegraphics[scale=0.4]{RPY2Matriz}
 	\caption[Relación entre RPY y la matriz rotacional]{Relación entre RPY y la matriz rotacional. }
 	\label{imagen:rpy2matriz}
 \end{figure} 
 
 Y de forma equivalente:
 

\begin{align}
 \psi \quad =\quad \arctan { (\frac { { r }_{ 21 } }{ { r }_{ 11 } }) } 
 \\ \theta \quad =\quad \arctan { (\frac { -{ r }_{ 31 } }{ \sqrt { { { r }_{ 32 } }^{ 2 }\quad +\quad { { r }_{ 33 } }^{ 2 } }  } ) } \quad 
 \\ \phi \quad =\quad \arctan { (\frac { { r }_{ 32 } }{ { r }_{ 33 } } ) } \quad 
 \label{eq:Matriz2RPY}
 \end{align}

\section{ Transformaciones de cuerpo rígido }

El movimiento de un robot en el espacio puede ser descrito mediante un movimiento rotacional y un movimiento traslacional. Una transformación de cuerpo rígido representa el cambio de movimiento rotacional y traslacional de un cuerpo respecto a otro sistema de referencia.

En aplicaciones de odometría y SLAM, las transformaciones de cuerpo rígido son utilizadas debido a que se asume que el entorno del robot se encuentra fijo, y por lo tanto, la posición y orientación del robot puede estar vinculada a un sistema de referencia fijo. Esto permite que el robot se pueda localizar respecto a su entorno y que se pueda obtener la trayectoria del robot como una concatenación de transformaciones de cuerpo rígido.

Considerar que partes del entorno se pueden movilizar aumenta en gran medida la complejidad de la estimación del movimiento del robot.

En síntesis, una transformación de cuerpo rígido $T$  es una matriz de 4x4 constituida por una matriz de rotación $R$ y un vector de traslación $t = {(tx, ty, tz)}^{T}$:

\begin{equation}
\begin{matrix} { { T } }\quad =\quad \begin{bmatrix} R & \quad { { t } } \\ { 0 }^{ T } & \quad 1 \end{bmatrix}\quad  \end{matrix}\quad =\quad \begin{bmatrix} { r }_{ 11 } & { \quad r }_{ 12 } & { \quad r }_{ 13 } & \quad t_{ x } \\ { r }_{ 21 } & { \quad r }_{ 22 } & { \quad r }_{ 23 } & { \quad t }_{ y } \\ { r }_{ 31 } & { \quad r }_{ 32 } & { \quad r }_{ 33 } & \quad { t }_{ z } \\ 0 & 0 & 0 & 1 \end{bmatrix}
\label{eq:TC}
\end{equation}

Esta matriz tiene en total 6 grados de libertad: tres grados pertenecientes a la rotación alrededor de los ejes $x$, $y$ y $z$, y tres grados de traslación $x$, $y$ y $z$. 

La representación del vector $t$ es canónica, ya que los tres componentes del vector corresponden directamente
a tres grados de libertad. Sin embargo, la representación R de la rotación no es
canónica, ya que posee nueve parámetros, pero en conjunto solo representan
tres grados de libertad. Los seis parámetros restantes están restringidos debido
a la característica de ortogonalidad de la matriz y la necesidad de tener norma
igual a 1 para todas sus columnas y filas (propiedades \ref{eq:Propiedad1MatrizRotacion} y \ref{eq:Propiedad2MatrizRotacion}). Esto impone seis restricciones, dejando
solamente tres parámetros libres de modificación.

En este caso, los puntos de ${\rm I\!R}^3$ de la forma $P = (X, Y, Z)$  son representados por vectores homogéneos $P = {(X, Y, Z, 1)}^{T}$ , y la multiplicación de $T$ por un vector homogéneo genera otro vector homogéneo:

\begin{equation}
\begin{pmatrix} \begin{matrix} X' \\ Y' \\ Z' \end{matrix} \\ 1 \end{pmatrix}\quad =\quad T\quad \begin{pmatrix} \begin{matrix} X \\ Y \\ Z \end{matrix} \\ 1 \end{pmatrix}
\end{equation}


Las transformaciones de cuerpo rígido se utilizan para representar el movimiento traslacional y rotacional de un objeto de manera compacta y siempre están definidas en relación con un sistema de coordenadas de referencia, ya se trate del sistema de coordenadas global o de otro sistema de coordenadas arbitrario. Esto permite concatenar varias transformaciones de cuerpo rígido aplicando la multiplicación matricial entre ellas:

\begin{equation}
{ T }_{ c }\quad =\quad { T }_{ a }{ .T }_{ a-b }.{ T }_{ b-c }
\end{equation}

En este caso, tal como se ilustra en la figura \ref{imagen:TC_Concatenacion}, la orientación y posición final del cuerpo ${T}_{c}$, respecto al sistema de referencia global, es el resultado de la concatenación de la orientación y posición inicial ${T}_{a}$ en $(a)$ con el cambio de orientación y traslación ${T}_{a-b}$  de $(a)$ a $(b)$, y el cambio ${T}_{b-c}$ de $(b)$ a $(c)$.


 \begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{TC_Concatenacion}
	\caption[Concatenación de transformaciones de cuerpo rígido]{Concatenación de transformaciones de cuerpo rígido}
	\label{imagen:TC_Concatenacion}
\end{figure} 

Las transformaciones de cuerpo rígido son fundamentales para poder relacionar las mediciones realizadas en el sistema de referencia de la imu con las imágenes tomadas por la cámara, y a su vez relacionar las estimaciones de movimiento de estos marcos de referencia con el sistema de referencia fijo.\\

La figura \ref{fig:TransformacionesRobot} presenta los sistemas de referencia presentes en el robot. La transformación ${T}_{IMU-CAM}$ relaciona los sistemas de referencia de la cámara y la imu, y corresponde a un parámetro extrínseco del sistema que es invariante debido a que la cámara y la imu se encuentran físicamente acoplados.Esta transformación se obtiene mediante la calibración de los sensores y esta conformada por dos partes: rotación del sistema de referencia de la cámara respecto al sistema de referencia de la imu; y traslación del sistema de referencia de la cámara respecto al sistema de referencia de la imu. Determinar estos parámetros con precisión es de vital importancia en los sistemas visuales-inerciales debido a que los errores de calibración puede generar resultados insatisfactorios en los algoritmos de estimación.



\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{ImuCam}
	\caption{Sistemas de Referencia del Robot }
	\label{fig:TransformacionesRobot}
\end{figure}


De forma concreta:

\begin{equation}
{ { T } }_{ IMU -CAM }\quad =\quad \begin{bmatrix} R_{ IMU - CAM } & \quad { { P } }_{ IMU - CAM } \\ { 0 }^{ T } & \quad 1 \end{bmatrix}\quad =\quad \begin{bmatrix} { r }_{ 11 } & { \quad r }_{ 12 } & { \quad r }_{ 13 } & \quad t_{ x-IMU } \\ { r }_{ 21 } & { \quad r }_{ 22 } & { \quad r }_{ 23 } & { \quad t }_{ y-IMU } \\ { r }_{ 31 } & { \quad r }_{ 32 } & { \quad r }_{ 33 } & \quad { t }_{ z-IMU } \\ 0 & 0 & 0 & 1 \end{bmatrix}\quad 
\label{eq:transformacionIMUCAM} 
\end{equation}

Donde ${R}_{IMU-CAM}$ es la matriz de rotación que al concantenarse con la orientación de la IMU, se obtiene la orientación de la cámara. La orientación de la IMU se representa mediante la matriz ${R}_{IMU}$, la cual es la orientación de la imu respecto al sistema de referencia fijo. En síntesis se tiene que la orientación de la cámara respecto al sistema de referencia fijo (${R}_{CAM}$) es:

\begin{equation}
{ R }_{ CAM }\quad =\quad { R }_{ IMU }*{ R }_{ IMU-CAM }\quad 
\label{eq:rotacionIMUCAM} 
\end{equation}


\section{Geometría epipolar y triangulación}

La geometría epipolar relaciona la localización en un par de imágenes de un punto $X$ en el espacio, y está relacionada directamente con la matriz esencial $E$, la cual a su vez es utilizada en métodos de estimación de la traslación y rotación basados en puntos característicos y en métodos de triangulación.

En primer lugar, se observa la figura \ref{imagen:epipolar}. En esta figura se observan la imagen 1 y la imagen 2, cuyos origines del sistema de referencia de la cámara se denotan como $O$ y $O'$, respectivamente.

El punto $X$ es proyectado en la imagen 1 en $x$. Podemos que también cada punto perteneciente a la recta $OX$ es proyectado al mismo punto $x$. Al considerar la imagen 2, se observa que la proyección del punto $X$ sobre la imagen es $x'$, pero que la posición en el plano de la imagen varia al cambiar la profundidad del punto $X$, es decir, al tomar otro punto en la recta $OX$.  Por tanto, la triangulación consiste en encontrar la profundidad correcta de $X$, conociendo las ubicaciones de sus proyecciones en un par de imágenes diferentes.

Las proyecciones de la recta $OX$ forma la linea sobre la imagen de la derecha, llamada epilinea. Se denomina epilinea del punto $x$ debido a que para encontrar este punto en la imagen 2 se debe buscar a lo largo de esta linea. Esto añade eficiencia computacional ya que no debemos buscar en la imagen entera, y a esta restricción se denomina restricción epipolar. El plano $XOO'$ es conocido como el plano epipolar.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{epipolar}
	\caption[Geometría epipolar]{Geometría epipolar. Adaptado \protect\footnotemark.}
	\label{imagen:epipolar}
\end{figure}
\footnotetext{\url{https://docs.opencv.org/3.1.0/da/de9/tutorial_py_epipolar_geometry.html}}

También es posible calcular el vector normal al plano epipolar utilizando los vectores paralelos a las rectas $OX$ y $O'X$. Para ello, se utilizan los segmentos de recta $Ox$ y $O'x'$, y se definen los vectores de características unitarios como:

\begin{equation}
\overset { \rightarrow  }{ { f }_{ i } } =\quad \left[ \begin{matrix} \frac { u_{ i }-{ c }_{ x } }{ { f }_{ x } }  \\ \frac { { v }_{ i }-{ c }_{ y } }{ { f }_{ y } }  \\ 1 \end{matrix} \right] 
\end{equation}

Este vector unitario puede ser entendido observando la ecuación \ref{eq:ReproyeccionCAM} como un vector que no posee unidades y que apunta desde el origen de coordenadas de la cámara a la ubicación del punto $X$ en el espacio. El vector ${ f }_{ 1}$ se obtiene a partir  de $x = (u1,v1)$  y de los parámetros intrínsecos de la cámara $fx, fy, cx$ y $cy$.

De esta forma es posible visualizar que el vector normal al plano epipolar puede ser obtenido con el producto vectorial de ${ f }_{ 1}$ y ${ f }_{ 2}$. Sin embargo, estos vectores no se encuentran referenciados a un único sistema de la cámara. El vector ${ f }_{ 1}$ se encuentra calculado en el sistema $O$ y el vector ${ f }_{ 2}$ al sistema $O'$. 

Por tanto colocamos el sistema de la imagen 1 como el sistema de referencia global, y calculamos  ${ f }_{ 2}$ 
conociendo la transformación de cuerpo rígido que existe de la imagen 1 a la imagen 2:

\begin{equation}
\begin{matrix} \overset { \rightarrow  }{ { f }_{ 2,\quad 1 } } \quad =\quad { R }_{ 1 }^{ 2 }.\overset { \rightarrow  }{ { f }_{ 2 } } +{ P }_{ 1 }^{ 2 } \end{matrix}
\end{equation}

Donde $\overset { \rightarrow  }{ { f }_{ 2,\quad 1 } } \quad $ significa el vector $\overset { \rightarrow  }{ { f }_{ 2 } } $ respecto al sistema de referencia de la imagen 1.

Sin embargo, debido a que sólo es necesaria la dirección de los vectores para el calculo del vector normal, y considerando que la traslación de los sistemas ${ P }_{ 1 }^{ 2 }$ no varia la dirección del vector $\overset { \rightarrow  }{ { f }_{ 2,\quad 1 } }$, el vector normal al plano epipolar puede ser calculado como:


\begin{equation}
\overset { \rightarrow  }{ n } \quad =\quad \overset { \rightarrow  }{ { f }_{ 1 } } \quad \times \quad (\quad \overset { \rightarrow  }{ { { R }_{ 1 }^{ 2 } } } \quad .\quad \overset { \rightarrow  }{ { f }_{ 2 } } \quad )
\label{eq:normalEpipolar}
\end{equation}


%Enlazar con ecuación de Z*vector
\section{ Filtros de fusión inercial }

Existen varias alternativas para determinar la orientación de un robot a partir de las medidas del acelerómetro y el giroscópio. Entre los más conocidos se encuentran el filtro Kalman, el filtro de Madgwick, el filtro complementario de Valenti, y el filtro de Mahony.

\subsubsection{Cálculo de Roll, Pitch y Yaw}
Lá forma básica de cálculo de los ángulos de roll, pitch y yaw es mediante el cálculo de la integral numérica de sus tasas de cambio como se muestra en
la integral definida en la ecuación 
\ref{eq:integracionAnguloDefinida} o en su integral numérica de la ecuación \ref{eq:integracionAnguloNumerica}.\\

\begin{equation} \label{eq:integracionAnguloDefinida}
\phi ({ t }_{ 1 })\quad =\int _{ { t }_{ 0 } }^{ { t }_{ 1 } }{ { \omega  }_{ \phi  }dt } \quad +\phi ({ t }_{ 0 })
\end{equation}

\begin{equation} \label{eq:integracionAnguloNumerica}
{ t }_{ 1 }\quad ={ { t }_{ 0 }+T*n) })\quad =\sum _{ p\quad =\quad k }^{ k+n-1 }{ \omega _{ \phi  }(p)T } \quad +\phi ({ t }_{ 0 })
\end{equation}

Donde:\\
$t_0 $: Tiempo inicial.\\
$t_1 $: Tiempo final (en el que se tomó la ultima medida de $\omega _{ \phi  }$).\\
$\phi ({ t }_{ 0 })$: Ángulo inicial.\\
$\phi ({ t }_{ 1 })$: Ángulo que se desea estimar.\\
$\omega _{ \phi  }$: Velocidad angular.\\
$T$: Periodo de muestreo de las medidas de velocidad angular.\\
$n$: Número de medidas de $\omega _{ \phi  }$.\\
$k$: k-ésima medida de $\omega _{ \phi  }$. \\


Debido a que el giroscopio mide la velocidad angular en cada eje de rotación respecto al sistema de referencia de la IMU, cada medida debe ser transformada a un marco de referencia fijo. Por lo tanto, en la aproximación básica de orientación se determina los valores de pitch y roll ($\phi$ y $\theta$) de la orientación del robot respecto al sistema fijo en el cual la gravedad es paralela al eje $z$ a partir de las medidas del acelerómetro y luego se obtienen las tasas de cambio de los ángulos de roll, pitch y yaw a través de la transformación de la ecuación \ref{eq:transformacionVelocidadAngular}.\\

\begin{equation}
\left[ \begin{matrix} \dot { \phi  }  \\ \dot { \theta  }  \\ \dot { \psi  }  \end{matrix} \right] =\left[ \begin{matrix} 1 & \quad \quad s(\phi )t(\theta ) & \quad c(\phi )t(\theta ) \\ 0 & c(\phi ) & -s(\phi ) \\ 0 & \frac { s(\phi ) }{ c(\theta ) }  & \frac { c(\phi ) }{ c(\theta ) }  \end{matrix} \right] \left[ \begin{matrix} { \omega  }_{ x } \\ { \omega  }_{ y } \\ { \omega  }_{ z } \end{matrix} \right] 
\label{eq:transformacionVelocidadAngular} 
\end{equation}

De esta forma se pueden obtener los ángulos de orientación integrando las velocidades angulares mediante la ecuación \ref{eq:integracionAnguloNumerica}. Sin embargo, debido a los bias del giroscopio, se propagaran errores de integración a medida que se incrementa el tiempo. Por tanto, se utilizan filtros de fusión que toman en cuanta tanto la estimación de los ángulos de roll y pitch provenientes del acelerómetro, como las tasas de cambio que se pueden obtener del roll, pitch y yaw  utilizando el giroscopio mediante la ecuación \ref{eq:transformacionVelocidadAngular}.\\

En el filtro Kalman por ejemplo, se utilizan
las observaciones de la variable a estimar y de su variación respecto al tiempo, para mejorar la estimación final. Esto ocurre debido a que las estimaciones de los ángulos de roll, pitch y yaw a partir del acelerómetro y el magnetómetro sufren de errores debido al ruido y bias inherentes a las unidades de medición inercial basadas en MEMS. Lo mismo ocurre con las medidas provenientes del giroscopio. Un filtro Kalman como el de la figura  \ref{fig:diagramaFiltroKalman} toma en cuenta estas medidas y sus errores para tener una mejor estimación de la orientación del robot.\\

Cabe destacar que las unidades de medición inercial más económicas no disponen de magnetómetro, por lo el cálculo  del ángulo de yaw se ve restringido a ser obtenido sólo a partir de la integración de su tasa de cambio, lo que conlleva a que cualquier filtro de fusión inercial sufra de drift  en el tiempo de este ángulo.


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{FiltroKalman}
	\caption{Diagrama básico del Filtro Kalman}
	\label{fig:diagramaFiltroKalman}
\end{figure}



Sin embargo, la represetanción de la transformación presentada en \ref{eq:transformacionVelocidadAngular} presenta singularidades debido a la representación basada en cosenos directores. Es por ello que filtros como el de Madgwick y el complementario utilizan cuaterniones en la estimación de la orientación.

\subsection{ Filtro de Madgwick }

Este algoritmo fue desarrollado por Sebastián Madgwick \cite{Madgwick} y utiliza la representación de la orientación basada en cuaterniones. Este filtro utiliza el algoritmo del gradiente descendente para calcular la dirección del error de medición a partir de los datos del giroscopio.\\

El filtro de Madgwick se encuentra dividido en cuator partes: el cálculo de la orientación a partir de las velocidades angulares medidas por el giroscopio; el calculo de las orientaciones a partir de los vectores medidos del campo gravitacional y del campo magnético; la fusión de las dos estimaciones anteriores, y finalmente la normalización del cuaternion de fusión resultante. Cuando no se tiene magnetómetro sólo son fusionados las medidas de aceleración y velocidad angular.

\subsection{ Filtro complementario }
Este filtro también utiliza la representación en cuaterniones. Fusiona la estimación del cuaternion de orientación con el delta de cuaternion obtenido del acelerómetro. Este último sirve sólo como corrección de las componentes de roll y pitch, estimando el ángulo de yaw a partir de las mediciones del giroscopio. Si se provee medidas del magnetómetro, se estima otro delta de cuaternion mediante las lecturas del campo  magnético, el cual se utiliza para las correcciones de la estimación del yaw.\\

El termino de filtro complemenetario se debe al uso del analisis en el domino de la frecuencia de las señales para tener una mejor estimación de una cantidad en particular. Si se tienen señales afectadas por  ruidos de diferentes frecuencias, se utilizan dos filtros con un ancho de banda apropiado. Para estimación de la orientación de las medidas de la IMU, un filtro complementario utiliza un filtro pasa altas para la orientación estimada del giroscopio, la cual es afectada por  ruido de baja frecuencia , y un filtro pasa bajo para las medidas del acelerómetro, las cuales poseen rudio de alta frecuencia. Idealmente, la fusión de los dos filtros genera un filtro pasa todo con medidas libres de ruido. El termino complementeario deriva debido a que la frecuencia de corte es la misma para ambos filtros. \\


%
\subsection{Metodología}

El proceso planteado para la comparación consiste en evaluar los parámetros antes mencionados para todas las combinaciones de extractores y emparejadores que se tienen, tal y como se ilustra en la figura \ref{imagen:comparacion}.

\begin{figure}[H]
	\centering
	\includegraphics[width=14cm]{comparacion.pdf}
	\caption[Combinación de algoritmos para el analisis de rendimiento]{Combinaciones posibles del módulo comparativo}
	\label{imagen:comparacion}
\end{figure}

Cuando se utiliza un algoritmo para emparejar características, es muy común que existan parejas erróneas, puesto que al tener una gran cantidad de datos, varios pares de descriptores pueden tener la similitud necesaria para ser considerados como el mismo punto. Por esta razón es importante emplear una etapa que permita filtrar dichas parejas. Como ya se mencionó, se tienen distintos tipos de emparejadores, y en función a cada uno es necesario realizar el descarte de manera distinta. A continuación se describe el proceso asociado a cada caso.

En el caso del algoritmo de fuerza bruta: Se obtiene la distancia de la mejor pareja, luego se descartan todas las parejas cuya distancia sea mayor que la mejor obtenida multiplicada por un factor de umbral. El proceso planteado se muestra en el algoritmo \ref{fuerzabruta}.

\begin{figure}[h]
	\centering
	\begin{minipage}{.7\linewidth}
		\begin{algorithm}[H] %or another one check
			\caption{Selección de buenas parejas - Fuerza bruta}
			\label{fuerzabruta}
			\SetAlgoLined
			$P{i}$ $\equiv$  $i$-ésima pareja\\
			$D_{i}$ $\equiv$ Distancia de la $i$-ésima pareja\\
			$N$ $\equiv$ Número de puntos emparejados\\
			$U$ $\equiv$ Umbral para descartar erróneos\\
			\Begin{
				$U = 0.8$\;
				\ForEach{i $\in$ n=1,2,$\cdots$,N}{
					\If{$D_{i} < mejorDistancia$}{
						$mejorDistancia = Dp_{i}$\;
					}
				}
				\ForEach{i $\in$ n=1,2,$\cdots$,N}{
					\If{$D_{i} > mejor distancia \cdot U$}{
						eliminar $P_{i}$\;
					}
				}
			}
		\end{algorithm}
	\end{minipage}
\end{figure}

En el caso del algoritmo bajo el esquema de vecinos mas cercanos: Se descarta cada pareja cuya distancia esté muy cercana a la distancia del vecino mas próximo. El proceso planteado se muestra en el algoritmo \ref{vecinosmascercanos}.

\begin{figure}[h]
	\centering
	\begin{minipage}{.75\linewidth}
		\begin{algorithm}[H] %or another one check
			\caption{Selección de buenas parejas - Vecinos mas cercanos}
			\label{vecinosmascercanos}
			\SetAlgoLined
			$P{i}$ $\equiv$ $i$-ésima pareja\\
			$D_{i}$ $\equiv$ Distancia de la $i$-ésima pareja\\
			$V_{i}$ $\equiv$ Distancia del vecino mas cercano de la $i$-ésima pareja\\
			$N$ $\equiv$ Número de puntos emparejados\\
			$U$ $\equiv$ Umbral para descartar erróneos\\
			\Begin{
				$U = 0.5$\;
				\ForEach{i $\in$ n=1,2,$\cdots$,N}{
					\If{$D_{i} > V_{i} \cdot U$}{
						eliminar $P_{i}$\;
					}
				}
			}
		\end{algorithm}
	\end{minipage}
\end{figure}

En segundo lugar, con el objetivo de mejorar la detección se implementa una etapa de pre-procesamiento en las imágenes de entrada. Tal y como se estudió en la sección teórica, para que un punto característico sea detectado, éste debe contener información que lo diferencie de su entorno cercano, en este caso esta diferencia es medida en función a su intensidad. En base a esta definición, se propone un algoritmo que permite mejorar el contraste en la imagen mediante la técnica de estirar su histograma. De esta forma cada imagen tendrá el máximo rango de excursión sobre las intensidades de sus puntos, permitiendo que los valores de umbral al detectar regiones, se superen con más facilidad.

Es importante destacar que en proceso todas las imágenes fueron almacenadas usando variables de 8 bits, es decir, que cada valor es representado en un rango comprendido entre 0 y 255.

Para el calculo del histograma se obtiene la frecuencia de aparición de cada valor de intensidad en la imagen. Luego, se obtiene el valor de la intensidad para la cual, la cantidad de píxeles cuyo valor sea menor o igual a ésta, no supere el 1\% de la cantidad total de píxeles en la imagen, este punto es llamado percentil bajo. A continuación se repite este proceso par ubicar el percentil alto, el cual corresponde con la intensidad para la cual la cantidad de píxeles inferiores a ésta, no supera el 99\% de la cantidad total de píxeles. Al tener estos valores se aplican los siguientes criterios:

\begin{itemize}
	\item Todos los píxeles cuyo valor se encuentre por debajo del percentil bajo es igualado a 0.
	\item Todos los píxeles cuyo valor se encuentre por encima del percentil alto es igualado a 255.
	\item Todos los valores cuyo valor se encuentre entre el percentil bajo y el alto, es reescalado usando la siguiente ecuación: 
	\begin{displaymath}
		I_{pixel} = \frac{(I_{pixel} - P_{bajo}) \cdot 255}{P_{alto} - P_{bajo}}
	\end{displaymath}
\end{itemize}



