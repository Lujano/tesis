\chapter{Odometría y SLAM}
\label{capitulo2}
\lhead{Capítulo 2. \emph{Odometría y SLAM}}

En este capítulo se presenta una revisión teórica del estado actual de las investigaciones que se han realizado en el área de Odometría visual, SLAM visual, y sus vertientes en las que se fusionan los datos inerciales.

\section{Estado del arte}

En los últimos años se han presentado diferentes alternativas para estimar de forma efectiva el movimiento que efectúa un robot y además lograr realizar un mapa de su entorno.


\section{SLAM visual}

\section{Fusión visual-inercial }
A lo largo de los años se han utilizado diferentes tipos de sensores y sus combinaciones para realizar la localización y mapeo simultáneo (SLAM) de un robot móvil en su entorno. Entre éstos se encuentran láseres de rango, sonares, sistemas de posicionamiento global (GPS), unidades de medición inercial (IMU) y cámaras monoculares, estereoscópicas, y RGB-D.\\

Cada tipología en la que son empleados estos sensores tiene sus limitaciones. Por ejemplo, los sistemas en que los se utiliza GPS están restringidos a ser utilizados al aire libre, lo que conlleva a que no puedan ser implementados en vehículos submarinos. Los láseres ofrecen información precisa del entorno pero tienen problemas en superficies reflectivas o absorbentes, además que pueden representar un alto costo,  y  ser lo suficientemente pesados como para ser descartados en aplicaciones con  vehículos aéreos. Por su parte, las medidas de sonares que son aplicados en espacios terrestres pueden tener alta incertidumbre ya que depende de la forma de las superficies y de su orientación relativa al sensor, mientras que las unidades de medición inercial presentan deriva y ruido en sus mediciones. En el caso de las cámaras, la  calidad de sus datos tienen una alta dependencia a las condiciones de iluminación. \\

La implementación de estas últimas en sistemas de localización y mapeo simultáneo ha tenido gran atención en los últimos años debido a su capacidad para capturar una gran cantidad de información sobre el entorno. Es por esto que se han desarrollado métodos de SLAM visual basados tanto en cámaras monoculares como estereoscópicas, utilizando para ello métodos directos, a través de la detección y emparejamiento de características en las imágenes; método indirectos, basados en el calculo del error fotométrico entre imágenes; y semi-directos, utilizando una combinación de métodos directos e indirectos.\\

Sin embargo, estos sistemas presentan debilidades. Los métodos de SLAM con cámaras monoculares presentan problemas de ambiguedad ante la escala, mientras que  los estereoscópicos están limitados por la relación entre la profundidad de la escena y la distancia entre las cámaras. \\

Es por ello que con el objetivo de generar un sistema más robusto en el que se puede complementar la información disponible entre sensores y compensar sus debilidades, se han propuesto esquemas basados en la fusión visual-inercial, en la que se pueden emplear cámaras estéreos o monoculares junto a una unidad de medición inercial, ofreciendo además una buena relación entre costo, peso, espacio ocupado y consumo de energía, que permite que puedan ser empleados en robots terrestres, aéreos y submarinos.\\

El núcleo de esta fusión se encuentra basado en la complementariedad que tienen estos sensores. Por ejemplo, las cámaras son precisas en movimientos lentos, en los cuales las medidas aceleración y velocidad angular provenientes de la IMU tienen mayor incertidumbre. En el caso de movimientos rápidos, sucede justo lo contrario: la IMU presenta menor incertidumbre en sus medidas mientras que las imágenes captadas por la cámara se distorsionan por efectos como el blur. \\

Por otro lado,  las frecuencias de muestreo de las cámaras convencionales están el orden de las decenas de Hertz, mientras que las de la IMU pueden alcanzar las centenas de Hertz, con lo que se  obtiene información inercial adicional entre dos imágenes, que al ser integrada permite establecer restricciones de movimiento,  logrando de esta forma alcanzar una mejor estimación del estado del robot frente a su entorno., requiriendo un menor tiempo de cómputo. \\
\section{Odometría visual}
\section{SLAM}


\subsection{OKVIS}

El sistema de SLAM visual-inercial basado en keyframes y optimización no lineal (OKVIS)  es un enfoque desarrollado en 2013 en el laboratorio d de sistemas autónomos de la Escuela Politécnica Federal de Zúrich (ETH), en Suiza.

Este enfoque integra de forma estrecha las medidas de la IMU y los keyframes provenientes de visión estéreo, utilizando optimización de Gauss-Newton para minimizar el error . El detector que se empleó es el detector de Harriz multiescala con optimización en el espacio SSE , combinado con el descriptor BRISK. 

La visión estereo es utilizada para realizar la triangulación de los puntos característicos en cada frame, los cuales son insertados a un mapa local. Luego se aplica el algoritmo de fuerza bruta para encontrar la correspondecia con el mapa global de landmarks. En este caso, se utiliza outlier rejection aplicnado es test de chi-cuadrado a las coordenadas de la imagen utilizando las poses obtenidas previamente mediante la integración de las medidas de la IMU.
En este enfoque se utiliza una ventana deslizante para mantener los frames más reciente, y la selección de keyframes se basa en una medida heuristica: si la razón entre el area ocupado por todo los puntos emparejados contra el area ocupaa por todos los puntos detectados está entre 50 a 60, el frame es considerado como keyframe.



Debido a que se utiliza keyframes, se pueden tener frames separados arbitrariamente en tiempo. Los keyfra

En este trabajo comparan las estimaciones realizadas entre los diferentes acoples: el visual, el ligeramente acoplado y el estrechamente acoplado.


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{EstadoDelArte/Rovio/RovioEstimacion.png}
	\caption{Captura del entorno de trabajo de la odometría visual-inercial. Las elipses encierran la zona donde se predice la localización de las características. Las elipses estrechas corresponden a las características de mayor incertidumbre, entre las que se encuentran las características nuevas. Luego de la estimación efectuada por el EKF, la localización de la característica es mostrada como un punto verde. Los números en color verde corresponden al número de veces que ha sido seguida la características (1 si es una nueva característica).}
	\label{fig:RovioEstimacion}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{EstadoDelArte/Rovio/RovioEstimacion.png}
	\caption{Captura del entorno de trabajo de la odometría visual-inercial. Las elipses encierran la zona donde se predice la localización de las características. Las elipses estrechas corresponden a las características de mayor incertidumbre, entre las que se encuentran las características nuevas. Luego de la estimación efectuada por el EKF, la localización de la característica es mostrada como un punto verde. Los números en color verde corresponden al número de veces que ha sido seguida la características (1 si es una nueva característica).}
	\label{fig:RovioEstimacion}
\end{figure}



\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{EstadoDelArte/Rovio/RovioEstimacion.png}
	\caption{Captura del entorno de trabajo de la odometría visual-inercial. Las elipses encierran la zona donde se predice la localización de las características. Las elipses estrechas corresponden a las características de mayor incertidumbre, entre las que se encuentran las características nuevas. Luego de la estimación efectuada por el EKF, la localización de la característica es mostrada como un punto verde. Los números en color verde corresponden al número de veces que ha sido seguida la características (1 si es una nueva característica).}
	\label{fig:OkvisResultados}
\end{figure}

\subsection{ROVIO}

La Odometría Visual-Inercial Robusta (ROVIO, del inglés: Robust Visual Inertial Odometry) es un algoritmo desarrollado en 2015 en el laboratorio de sistemas autónomos del ETH.

Este algoritmo utilizan directamente como error la intensidad de los píxeles, utilizando parches en las  imágenes y aplicando la estructura piramidal de 4 niveles y manteniendo un alto nivel de robustez. Los parches utilizados están fuertemente acoplados con un filtro Kalman Extendido (EKF, del inglés: Extended Kalman Filters) y los landmarks 3D son siempre estimados con respecto a la pose actual de la cámara. lo que es conocido como el enfoque robocéntrico. Además, estos landmarks son parametrizados para lograr una na forma compacta de representación y de esa forma mejorar el desempeño computacional del algoritmo. 

El filtro empleado fusiona los datos de aceleración y velocidad angular de la IMU, y los parámetros extrinsecos de la cámara y los biases de la IMU son coestimados. 

Las características utilizadas aquí se refieren directamente a un píxel dentro de un parche de la imagen. El propósito del filtro es predecir la ubicación de las características en la siguiente imagen y de esta forma, extraer un parche de la siguiente imagen, y estimar la mejor transformación de cuerpo rígido que minimice el error de intesidad entre los parches. 

Cuando la predicción de la ubicación del parche se encuentra fuera de la región de la imagen, es eliminada la características y se crean nuevas. También puede ser eliminada la caracteristica en función de sus estadísticas, las cuales están asociadas al grado de certidumbre o varianza. La imagen \ref{fig:RovioEstimacion} muestra un ejemplo de esto.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{EstadoDelArte/Rovio/RovioEstimacion.png}
	\caption{Captura del entorno de trabajo de la odometría visual-inercial. Las elipses encierran la zona donde se predice la localización de las características. Las elipses estrechas corresponden a las características de mayor incertidumbre, entre las que se encuentran las características nuevas. Luego de la estimación efectuada por el EKF, la localización de la característica es mostrada como un punto verde. Los números en color verde corresponden al número de veces que ha sido seguida la características (1 si es una nueva característica).}
	\label{fig:RovioEstimacion}
\end{figure}


Este enfoque no requiere una etapa de inicialización, por lo que es posible utilizar este sistema de estimación de estados directamente.


Para evaluar la robustez de este algoritmo, el laboratorio utilizo su propio vehiculo aerio, equipado con dos cámaras con disparo global sincronizadas con el trigger de la imu. En el contexto del trabajo sólo fue utilizado una de las cámaras. El Ground truth fue proveido por un sistema externo de captura de movimiento. En esta configuración se fijó la tasa de las medidas de la IMU a 200Hz y la de las cámaras a 20Hz. Los resultados de la estimación se presentan en las figuras \ref{fig:RovioOrientacion} y  \ref{fig:RovioTrayectoria}.



\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{EstadoDelArte/Rovio/RovioOrientacion.png}
	\caption{Ángulos RPY estimados (rojo) del UAV comparado con los obtenidos mediante el sistema de captura de movimiento (azul). La incertidumbre de la estimación se muestra con lineas discontinuas. }
	\label{fig:RovioOrientacion}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{EstadoDelArte/Rovio/RovioTrayectoria.png}
	\caption{Trayectoria estimada (rojo) del UAV comparado con el groundtruth (azul) proveniente del sistema de captura de movimiento .}
	\label{fig:RovioTrayectoria}
\end{figure}




\section{Fusión visual-inercial }
A lo largo de los años se han utilizado diferentes tipos de sensores y sus combinaciones para realizar la localización y mapeo simultáneo (SLAM) de un robot móvil en su entorno. Entre éstos se encuentran láseres de rango, sonares, sistemas de posicionamiento global (GPS), unidades de medición inercial (IMU) y cámaras monoculares, estereoscópicas, y RGB-D.\\

Cada tipología en la que son empleados estos sensores tiene sus limitaciones. Por ejemplo, los sistemas en que los se utiliza GPS están restringidos a ser utilizados al aire libre, lo que conlleva a que no puedan ser implementados en vehículos submarinos. Los láseres ofrecen información precisa del entorno pero tienen problemas en superficies reflectivas o absorbentes, además que pueden representar un alto costo,  y  ser lo suficientemente pesados como para ser descartados en aplicaciones con  vehículos aéreos. Por su parte, las medidas de sonares que son aplicados en espacios terrestres pueden tener alta incertidumbre ya que depende de la forma de las superficies y de su orientación relativa al sensor, mientras que las unidades de medición inercial presentan deriva y ruido en sus mediciones. En el caso de las cámaras, la  calidad de sus datos tienen una alta dependencia a las condiciones de iluminación. \\

La implementación de estas últimas en sistemas de localización y mapeo simultáneo ha tenido gran atención en los últimos años debido a su capacidad para capturar una gran cantidad de información sobre el entorno. Es por esto que se han desarrollado métodos de SLAM visual basados tanto en cámaras monoculares como estereoscópicas, utilizando para ello métodos directos, a través de la detección y emparejamiento de características en las imágenes; método indirectos, basados en el calculo del error fotométrico entre imágenes; y semi-directos, utilizando una combinación de métodos directos e indirectos.\\

Sin embargo, estos sistemas presentan debilidades. Los métodos de SLAM con cámaras monoculares presentan problemas de ambiguedad ante la escala, mientras que  los estereoscópicos están limitados por la relación entre la profundidad de la escena y la distancia entre las cámaras. \\

Es por ello que con el objetivo de generar un sistema más robusto en el que se puede complementar la información disponible entre sensores y compensar sus debilidades, se han propuesto esquemas basados en la fusión visual-inercial, en la que se pueden emplear cámaras estéreos o monoculares junto a una unidad de medición inercial, ofreciendo además una buena relación entre costo, peso, espacio ocupado y consumo de energía, que permite que puedan ser empleados en robots terrestres, aéreos y submarinos.\\

El núcleo de esta fusión se encuentra basado en la complementariedad que tienen estos sensores. Por ejemplo, las cámaras son precisas en movimientos lentos, en los cuales las medidas aceleración y velocidad angular provenientes de la IMU tienen mayor incertidumbre. En el caso de movimientos rápidos, sucede justo lo contrario: la IMU presenta menor incertidumbre en sus medidas mientras que las imágenes captadas por la cámara se distorsionan por efectos como el blur. \\

Por otro lado,  las frecuencias de muestreo de las cámaras convencionales están el orden de las decenas de Hertz, mientras que las de la IMU pueden alcanzar las centenas de Hertz, con lo que se  obtiene información inercial adicional entre dos imágenes, que al ser integrada permite establecer restricciones de movimiento,  logrando de esta forma alcanzar una mejor estimación del estado del robot frente a su entorno., requiriendo un menor tiempo de cómputo. \\
\section{EuRoC MAV Dataset}
El conjunto de datos de prueba empleado en este trabajo corresponde al al EuRoC MAV Dataset \cite{0}, el cual es un dataset de referencia que es utilizado para evaluar diferentes algoritmos de SLAM visual-inercial y que dispone de 11 secuencias de datos. \\


El EuRoC MAV Dataset contiene imágenes estéreo sincronizadas con las medidas de la IMU,  y un groundtruth preciso que es estimado utilizando el láser Leica y el sistema de captura de movimiento Vicon. Los datos fueron recolectados utilizando robot aéreo el cual se presenta en la figura \ref{fig:robotEuroc}. \\

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Implementacion/robotEuroc.png}
	\caption{Robot aéreo empleado en la recolección de datos en el EuRoC MAV Dataset .}
	\label{fig:robotEuroc}
\end{figure}

Las cámaras empleadas en estas secuencias capturan datos a 20Hz, mientras que la IMU opera a 200Hz. En nuestro caso, se utilizará la cámara CAM0 de la figura \ref{fig:robotEuroc}. Esta cámara se encuentra sincronizada con la IMU tal como se presenta en la figura \ref{fig:sincronizacionEuroc}. En un tiempo $t_k$ se captura tanto la imagen de la cámara, como las medidas de aceleración y velocidad angular provenientes de la IMU. El tiempo $t_c$ representa el lapso de tiempo entre la captura de dos imágenes y corresponde con 50 ms. En este caso, se tienen $n = 10$ medidas de la IMU entre cada captura de la cámara. \\

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Implementacion/DiagramaIMUCamara.png}
	\caption{Sincronización entre la IMU y la cámara.}
	\label{fig:sincronizacionEuroc}
\end{figure}

También se dispone de la calibración de los parámetros intrísecos y extrinsecos del sistema. En la figura \ref{fig:extrinsecosEuroc} se muestran los parámetros extrínsecos relevantes para este trabajo. En este caso, se utiliza la transformación de cuerpo rígido $T{B-CAM0}$, que relaciona el sistema de referencia de la IMU (Body, B) y el sistema de referencia de la cámara CAM0. 



\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Euroc/Enlaces.png}
	\caption{Parámetros extrínsecos del sistema}
	\label{fig:extrinsecosEuroc}
\end{figure}

Las secuencias de este conjuntos de datos se encuentran clasificadas en "fácil", "media", y "difícil",  en función de la media de la velocidad lineal y angular del robot utilizado y de las condiciones visuales y niveles de iluminación. En la figura \ref{fig:tablaDeCaracteristicas} se presentan las características de las secuencias. 

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Euroc/TabladeCaracteristicas.png}
	\caption{Características de las secuencias de datos del EuRoC MAV Dataset}
	\label{fig:tablaDeCaracteristicas}
\end{figure}



Este conjunto de datos fue recolectado en dos tipos de ambiente. El primer ambiente se presenta en la figura \ref{fig:machineHall} y  corresponde a una sala de máquinas en el cual fueron recolectadas 5 secuencias de datos (MH\_01\_easy, MH\_02\_easy, MH\_03\_medium, MH\_04\_difficult, MH\_05\_difficult). \\

En la figura \ref{fig:vicon} se muestra el segundo ambiente correspondiente a una habitación en el que se tomaron 6 secuencias de datos (V1\_01\_easy, V1\_02\_medium, V1\_03\_difficult, V2\_02\_easy, V2\_02\_medium, V2\_03\_difficult). En estas secuencias se dispone de la nube de puntos de la habitación, la cual fue generada a través de la fusión de las medidas tomadas con el láser Leica, y se presentan en la figura \ref{fig:pointcloudEuroc}.\\

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.2]{Euroc/MachineHall1.png}
	\includegraphics[scale=0.2]{Euroc/MachineHall2.png}
	\caption{Imágenes de la secuencia de datos correspondiente a la sala de máquinas.}
	\label{fig:machineHall}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.2]{Euroc/Vicon0.png}
	\includegraphics[scale=0.2]{Euroc/Vicon1.png}
	\caption{Imágenes de la secuencia de datos correspondiente a la habitación Vicon.}
	\label{fig:vicon}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.2]{Euroc/Pointcloud0.png}
	\includegraphics[scale=0.2]{Euroc/Pointcloud1.png}
	\includegraphics[scale=0.2]{Euroc/Pointcloud2.png}
	\includegraphics[scale=0.2]{Euroc/Pointcloud3.png}
	\caption{Nubes de puntos de la habitación utilizada en el EuRoC MAV Dataset}
	\label{fig:pointcloudEuroc}
\end{figure}

.


\section{Librería de desarrollo}

\subsection{OpenCV}

\begin{wrapfigure}{R}{5cm}
	\begin{center}
		\vspace*{-0.2in}
		\includegraphics[width=4cm]{opencv}
	\end{center}
	\caption{Logo de la librería OpenCV}
\end{wrapfigure}

Para la implementación de los módulos previamente descritos, es necesario el uso de librerías y entornos de trabajo, que permitan un manejo eficiente de las imágenes. Además, el uso de de plataformas que se encuentren estandarizadas en esta área de estudio, facilita que el desarrollo del presente trabajo siga avanzando de la mano de futuros desarrolladores. En base a esto, se seleccionó la librería OpenCV\footnote{\url{http://opencv.org/}} para la implementación de los módulos necesarios en el sistema de generación de mosaico.

OpenCV (del inglés: \textit{Open Source Computer Vision}) es una librería de procesamiento de imágenes desarrollada por la empresa Intel\footnote{\url{http://www.intel.com}} en el año 1999. Esta librería ofrece un gran numero de algoritmos optimizados (actualmente mas de 2.500), el cual proporciona un entorno de desarrollo altamente eficiente para aplicaciones de procesamiento de imágenes. Asimismo presenta una gran aceptación por parte de los usuarios en el mundo académico y comercial, con mas de 47 mil usuarios activos, y un numero de descargas que supera los 14 millones. Se considera el estándar de facto en la comunidad de desarrolladores, y en especial para proyectos de investigación en procesamiento de imágenes y visión por computadora. 

Esta plataforma tiene soporte para distintos sistemas operativos, como \textit{Windows, Linux, Mac OS, iOS y Android.} Además de tener la posibilidad de trabajarla con diversos lenguajes de programación como: C++, Python, JavaScript. La motivación del presente trabajo se encuentra orientada al desarrollo de una aplicación que en un futuro pueda ser embebida en un sistema de navegación automático, con lo cual el soporte de un lenguaje de bajo nivel como C++, puede permitir el desarrollo de un algoritmo con suficiente velocidad de cómputo para este fin.

Por otra parte, esta librería presenta soporte para trabajar con la arquitectura de cálculo paralelo \textit{CUDA} (del inglés: \textit{Compute Unified Device Architecture}) de la empresa NVIDIA\footnote{\url{http://www.nvidia.com}}, con la cual se puede aprovechar el uso de las unidades de procesamiento gráfico para acelerar el rendimiento del algoritmo que se implemente. Cabe destacar que los equipos presentes en el laboratorio del \textit{GIDM} tienen disponible tarjetas gráficas con este soporte.

